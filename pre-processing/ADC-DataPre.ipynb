{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amex-Default-Challange.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1TepCm9tdMNCseL7ukvBGWHDbAfunPKC2",
      "authorship_tag": "ABX9TyP7ChGoWprcJGz2WU4LAnz5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eutiagovski/amex-default-predicition/blob/main/pre-processing/ADC-DataPre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACKNOWLEDGEMENTS\n",
        "\n",
        "    This notebook was inspired on @susnato\n",
        "\n",
        "    Because the originl data are too large to fit in memory, I am using the dataset of Feather & PArquet files by @RADDAR\n",
        "\n",
        "    The Techniques I used to reduce the size of the data is inspired from this great discussion by @cdeotte\n",
        "\n",
        "    Many of the techniques related to Numerical Features are inspired from this awesome notebook by @lucasmorin\n"
      ],
      "metadata": {
        "id": "6mT26vYshdQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### imports"
      ],
      "metadata": {
        "id": "eQDK9BdpiG10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('max_rows', 100)\n",
        "pd.set_option('max_columns', 300)"
      ],
      "metadata": {
        "id": "dN0v3b-qhbHN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###loading data"
      ],
      "metadata": {
        "id": "yUmz27SGic3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:\n",
        "\n",
        "    D_* = Delinquency variables\n",
        "    S_* = Spend variables\n",
        "    P_* = Payment variables\n",
        "    B_* = Balance variables\n",
        "    R_* = Risk variables\n",
        "\n",
        "with the following features being categorical:\n",
        "\n",
        "['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "\n",
        "Your task is to predict, for each customer_ID, the probability of a future payment default (target = 1)."
      ],
      "metadata": {
        "id": "vgWeOJSkmqwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = pd.read_parquet(\"/content/drive/MyDrive/kaggle/amex-default-predict/train.parquet\")\n",
        "train_labels = pd.read_csv(\"/content/drive/MyDrive/kaggle/amex-default-predict/train_labels.csv\")\n",
        "test_set = pd.read_parquet(\"/content/drive/MyDrive/kaggle/amex-default-predict/test.parquet\")\n",
        "submission = pd.read_csv('/content/drive/MyDrive/kaggle/amex-default-predict/sample_submission.csv')"
      ],
      "metadata": {
        "id": "yZgpESj9iiPK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###features engine\n"
      ],
      "metadata": {
        "id": "T7Hza92znBHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### preprocessing data (https://www.kaggle.com/code/susnato/amex-data-preprocesing-feature-engineering)\n",
        "\n",
        "print(train_set.shape, train_labels.shape)\n",
        "print(test_set.shape, submission.shape)\n",
        "\n",
        "bin_cols = ['B_31', 'D_87']\n",
        "cat_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
        "num_cols = list(set(train_set.columns)-set(cat_cols+['S_2', 'customer_ID']))\n",
        "\n",
        "int8_num_cols = list(set(train_set.dtypes[train_set.dtypes==np.int8].axes[0]) - set(cat_cols))\n",
        "int16_num_cols = list(set(train_set.dtypes[train_set.dtypes==np.int16].axes[0]) - set(cat_cols))\n",
        "float32_num_cols = list(set(train_set.dtypes[train_set.dtypes==np.float32].axes[0]) - set(cat_cols))\n",
        "\n",
        "def last_2(series):\n",
        "    return series.values[-2] if len(series.values)>=2 else -127\n",
        "\n",
        "def last_3(series):\n",
        "    return series.values[-3] if len(series.values)>=3 else -127\n",
        "\n",
        "\n",
        "print(\"We have {} Categorical features and {} Numerical features\".format(len(cat_cols), len(num_cols)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A_Xv4hPnEf6",
        "outputId": "71de371e-1a1b-492e-823d-c14416a4fb90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5531451, 190) (458913, 2)\n",
            "(11363762, 190) (924621, 2)\n",
            "We have 11 Categorical features and 177 Numerical features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Encode customer ids(https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054)\n",
        "\n",
        "train_set['customer_ID'] = train_set['customer_ID'].apply(lambda x: int(x[-16:], 16)).astype(np.int64)\n",
        "train_labels['customer_ID'] = train_labels['customer_ID'].apply(lambda x: int(x[-16:], 16)).astype(np.int64)\n",
        "# test_set['customer_ID'] = test_set['customer_ID'].apply(lambda x: int(x[-16:], 16)).astype(np.int64)\n",
        "# submission['customer_ID'] = submission['customer_ID'].apply(lambda x: int(x[-16:], 16)).astype(np.int64)"
      ],
      "metadata": {
        "id": "1NWjUvuNoO5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### features engine (https://www.kaggle.com/code/susnato/amex-data-preprocesing-feature-engineering)\n",
        "\n",
        "def take_first_col(series):\n",
        "    return series.values[0]\n",
        "\n",
        "def prepare_date_features(df):\n",
        "    ### Drop all other columns except the S_2 and customer_ID(cat_cols, num_cols)\n",
        "    df = df.drop(cat_cols+num_cols, axis=1)\n",
        "    \n",
        "    ### Converting S_2 column to datetime column\n",
        "    df['S_2'] = pd.to_datetime(df['S_2'])\n",
        "\n",
        "    ### How many rows of records does each customer has?\n",
        "    df['rec_len'] = df[['customer_ID', 'S_2']].groupby(by=['customer_ID'])['S_2'].transform('count').astype(np.int8)\n",
        "\n",
        "    ### Encode the 1st statement and the last statement time\n",
        "    df['S_2_first'] = df[['customer_ID', 'S_2']].groupby(by=['customer_ID'])['S_2'].transform('min')\n",
        "    df['S_2_last'] = df[['customer_ID', 'S_2']].groupby(by=['customer_ID'])['S_2'].transform('max')\n",
        "\n",
        "    ### For how long(days) the customer is receiving the statements\n",
        "    df['S_2_period'] = (df[['customer_ID', 'S_2']].groupby(by=['customer_ID'])['S_2'].transform('max') - df[['customer_ID', 'S_2']].groupby(by=['customer_ID'])['S_2'].transform('min')).dt.days.astype(np.float16)\n",
        "\n",
        "    ### Days Between 2 statements \n",
        "    df['days_between_statements'] = df[['customer_ID', 'S_2']].sort_values(by=['customer_ID', 'S_2']).groupby(by=['customer_ID'])['S_2'].transform('diff').dt.days.astype(np.float16)\n",
        "    df['days_between_statements'] = df['days_between_statements'].fillna(0)\n",
        "    df['days_between_statements_mean'] = df[['customer_ID', 'days_between_statements']].sort_values(by=['customer_ID', 'days_between_statements']).groupby(by=['customer_ID']).transform('mean').astype(np.float16)\n",
        "    df['days_between_statements_std'] = df[['customer_ID', 'days_between_statements']].sort_values(by=['customer_ID', 'days_between_statements']).groupby(by=['customer_ID']).transform('std').fillna( df['days_between_statements_mean']).astype(np.float16)\n",
        "    df['days_between_statements_max'] = df[['customer_ID', 'days_between_statements']].sort_values(by=['customer_ID', 'days_between_statements']).groupby(by=['customer_ID']).transform('max').astype(np.float16)\n",
        "    df['days_between_statements_min'] = df[['customer_ID', 'days_between_statements']].sort_values(by=['customer_ID', 'days_between_statements']).groupby(by=['customer_ID']).transform('min').astype(np.float16)\n",
        "\n",
        "    ### https://www.kaggle.com/code/lucasmorin/amex-lgbm-features-eng/notebook\n",
        "    df['S_2'] = (df['S_2_last']-df['S_2']).dt.days.astype(np.float16)\n",
        "\n",
        "    ### Difference between S_2_last(max) and S_2_last \n",
        "    df['S_2_last_diff'] = (df['S_2_last'].max()-df['S_2_last']).dt.days.astype(np.float16)\n",
        "\n",
        "    ### Difference between S_2_first(min) and S_2_first \n",
        "    df['S_2_first_diff'] = (df['S_2_first'].min()-df['S_2_first']).dt.days.astype(np.float16)\n",
        "\n",
        "    ### Get the (day,month,year) and drop the S_2_first because we can't directly use them\n",
        "    df['S_2_first_dd'] = df['S_2_first'].dt.day.astype(np.int8)\n",
        "    df['S_2_first_mm'] = df['S_2_first'].dt.month.astype(np.int8)\n",
        "    df['S_2_first_yy'] = df['S_2_first'].dt.year.astype(np.int8)\n",
        "    \n",
        "    df['S_2_last_dd'] = df['S_2_last'].dt.day.astype(np.int8)\n",
        "    df['S_2_last_mm'] = df['S_2_last'].dt.month.astype(np.int8)\n",
        "    df['S_2_last_yy'] = df['S_2_last'].dt.year.astype(np.int8)\n",
        "    \n",
        "    agg_df = df.groupby(by=['customer_ID']).agg({'S_2':['last', last_2, last_3],\n",
        "                                                 'days_between_statements':['last', last_2, last_3]})\n",
        "    agg_df.columns = [i+'_'+j for i in ['S_2', 'days_between_statements'] for j in ['last', 'last_2', 'last_3']]\n",
        "    df = df.groupby(by=['customer_ID']).agg(take_first_col)\n",
        "    df = df.merge(agg_df, how='inner', left_index=True, right_index=True)\n",
        "    df = df.drop(['S_2', 'days_between_statements', 'S_2_first', 'S_2_last_x'], axis=1)\n",
        "\n",
        "    return df \n"
      ],
      "metadata": {
        "id": "4p-xI-vHpbMj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_numerical_features(df):\n",
        "    for num_c in list(num_cols):\n",
        "        col_dtype = df[num_c].dtype\n",
        "        df[num_c] = df[num_c].fillna(df[num_c].mean())\n",
        "        df[num_c] = df[num_c].astype(col_dtype)\n",
        "    \n",
        "    df['S_2'] = pd.to_datetime(df['S_2'])\n",
        "    df = df.sort_values(by=['customer_ID', 'S_2'])\n",
        "    ### Drop cat columns and S_2 so that you only have num features and customer_ID\n",
        "    df = df.drop(cat_cols+['S_2'], axis=1)\n",
        "    num_feature_list = ['min', 'max', 'mean', 'std', 'last', last_2, last_3]\n",
        "    \n",
        "    df_float32_agg = df[['customer_ID']+float32_num_cols].groupby(by=['customer_ID']).agg(num_feature_list).astype(np.float32)\n",
        "    df_float32_agg.columns = [str(c[0])+'_'+str(c[1]) for c in df_float32_agg.columns]\n",
        "    \n",
        "    df_int_agg = df[['customer_ID']+int8_num_cols+int16_num_cols].groupby(by=['customer_ID']).agg(num_feature_list).astype(np.float16)\n",
        "    df_int_agg.columns = [str(c[0])+'_'+str(c[1]) for c in df_int_agg.columns]\n",
        "    \n",
        "    #df_agg = df.groupby(by=['customer_ID']).agg(num_feature_list).astype(np.float32)\n",
        "    #df_agg.columns = [str(c[0])+'_'+str(c[1]) for c in df_agg.columns]\n",
        "    df_agg = df_float32_agg.merge(df_int_agg, left_index=True, right_index=True)\n",
        "    df_agg[[ii+'_last' for ii in int8_num_cols]] = df_agg[[ii+'_last' for ii in int8_num_cols]].astype(np.int8)\n",
        "    df_agg[[ii+'_last_2' for ii in int8_num_cols]] = df_agg[[ii+'_last_2' for ii in int8_num_cols]].astype(np.int8)\n",
        "    df_agg[[ii+'_last_3' for ii in int8_num_cols]] = df_agg[[ii+'_last_3' for ii in int8_num_cols]].astype(np.int8)\n",
        "    \n",
        "    del df, df_float32_agg, df_int_agg\n",
        "    gc.collect()\n",
        "    return df_agg\n",
        "\n"
      ],
      "metadata": {
        "id": "gNR1eA1x-JFE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_cat_features(df):\n",
        "    remove = ['customer_ID']\n",
        "\n",
        "    agg_dict_num = {}\n",
        "    agg_dict_cat = {}\n",
        "\n",
        "    mean_diff = lambda x: np.nanmean(np.diff(x.values))\n",
        "    mean_diff.__name__ = 'mean_diff'\n",
        "\n",
        "    for c in df.columns:\n",
        "        if c not in remove:\n",
        "            if c not in cat_cols+bin_cols:\n",
        "                agg_dict_num[c] = ['mean','std','min','max','last', last_2, last_3]\n",
        "            else:\n",
        "                agg_dict_cat[c] = ['nunique', ] \n",
        "    \n",
        "    df.loc[:,cat_cols+bin_cols] = df.loc[:,cat_cols+bin_cols].astype(str)\n",
        "    df_agg = df.groupby('customer_ID').agg(agg_dict_cat)\n",
        "    df_agg.columns = [str(c[0])+'_'+str(c[1]) for c in df_agg.columns]\n",
        "    df_list = []\n",
        "    for c in cat_cols+bin_cols:\n",
        "        df_cat = df.groupby(['customer_ID',c])[c].count()\n",
        "        df_cat = df_cat.unstack()\n",
        "        df_cat.columns = [df_cat.columns.name + '_' + c for c in df_cat.columns]\n",
        "        df_cat = df_cat.fillna(0)\n",
        "        df_list.append(df_cat)\n",
        "    df_out = pd.concat([df_agg]+df_list, axis=1)\n",
        "    df_out = df_out.fillna(np.nanmean(df_out))\n",
        "    \n",
        "    del df\n",
        "    gc.collect()\n",
        "    return df_out\n",
        "\n"
      ],
      "metadata": {
        "id": "o_3dQr0P-LAE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skfolds = StratifiedKFold(n_splits=6)\n",
        "train_5_fold_splits = []\n",
        "\n",
        "for train_index, test_index in skfolds.split(train_labels['customer_ID'], train_labels['target']):\n",
        "  print(train_labels.iloc[test_index]['target'].value_counts())\n",
        "  train_5_fold_splits.append(train_labels.iloc[test_index]['customer_ID'])\n",
        "\n",
        "train_labels = train_labels.set_axis(train_labels['customer_ID'])\n",
        "train_labels = train_labels.drop(['customer_ID'], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd2WTv2hCsV_",
        "outputId": "1187f914-26c5-4287-9ea2-93e58b096393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    85022\n",
            "1    29707\n",
            "Name: target, dtype: int64\n",
            "0    85021\n",
            "1    29707\n",
            "Name: target, dtype: int64\n",
            "0    85021\n",
            "1    29707\n",
            "Name: target, dtype: int64\n",
            "0    85021\n",
            "1    29707\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split train\n",
        "\n",
        "for (i,ids) in enumerate(train_5_fold_splits):\n",
        "    # if i == 1:\n",
        "    #   break\n",
        "    print(i, len(ids))\n",
        "    train_data_part = train_set[train_set.customer_ID.isin(ids)].sort_values(by=['customer_ID'])\n",
        "    train_data_time = prepare_date_features(train_data_part).sort_values(by=['customer_ID'])\n",
        "    train_data_num = prepare_numerical_features(train_data_part).sort_values(by=['customer_ID'])\n",
        "    train_data_cat = prepare_cat_features(train_data_part).sort_values(by=['customer_ID'])\n",
        "    \n",
        "    assert list(train_data_time.axes[0])==list(train_data_num.axes[0])==list(train_data_cat.axes[0])\n",
        "\n",
        "    y = train_labels.loc[ids]['target']\n",
        "    x = train_data_time.merge(train_data_cat, left_index=True, right_index=True).merge(train_data_num, left_index=True, right_index=True)\n",
        "    \n",
        "    ### Save to Pickle\n",
        "    train_data_time.merge(train_data_cat, left_index=True, right_index=True).merge(train_data_num, left_index=True, right_index=True).to_pickle('/content/drive/MyDrive/kaggle/amex-default-predict/train_data_{}.pkl'.format(i))\n",
        "    np.save(\"/content/drive/MyDrive/kaggle/amex-default-predict/train_y_{}.npy\".format(i), y)\n",
        "\n",
        "    # callback_to_my_function\n",
        "\n",
        "    del train_data_time, train_data_num, train_data_cat, train_data_part, y\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNGYbkPPCvFR",
        "outputId": "3eec121d-b911-43d0-a793-a0914efe3d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 114729\n",
            "1 114728\n",
            "2 114728\n",
            "3 114728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split test\n",
        "# https://stackoverflow.com/questions/2130016/splitting-a-list-into-n-parts-of-approximately-equal-length\n",
        "\n",
        "def split(a, n):\n",
        "    k, m = divmod(len(a), n)\n",
        "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
        "\n",
        "test_split_ids = split(test_set.customer_ID.unique(),12)\n",
        "\n",
        "for (i,ids) in enumerate(test_split_ids):\n",
        "    print(i, len(ids))\n",
        "    test_data_part = test_set[test_set.customer_ID.isin(ids)].sort_values(by=['customer_ID'])\n",
        "    \n",
        "    test_data_time = prepare_date_features(test_data_part).sort_values(by=['customer_ID'])\n",
        "    test_data_num = prepare_numerical_features(test_data_part).sort_values(by=['customer_ID'])\n",
        "    test_data_cat = prepare_cat_features(test_data_part).sort_values(by=['customer_ID'])\n",
        "    assert list(test_data_part.axes[0])==list(test_data_part.axes[0])==list(test_data_part.axes[0])\n",
        "    ### Save to Pickle\n",
        "    test_data_time.merge(test_data_cat, left_index=True, right_index=True).merge(test_data_num, left_index=True, right_index=True).to_pickle('/content/drive/MyDrive/kaggle/amex-default-predict/test_data_{}.pkl'.format(i))\n",
        "\n",
        "    del test_data_time, test_data_num, test_data_cat, test_data_part\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Fx3St6mkgq",
        "outputId": "da59c975-14a2-43f3-8c22-306a4a2d8687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 77052\n",
            "1 77052\n",
            "2 77052\n",
            "3 77052\n",
            "4 77052\n"
          ]
        }
      ]
    }
  ]
}